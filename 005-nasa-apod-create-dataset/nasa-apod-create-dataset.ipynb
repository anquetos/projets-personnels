{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des librairies\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import re\n",
    "from PIL import Image, ExifTags\n",
    "Image.MAX_IMAGE_PIXELS = 933120000\n",
    "from io import BytesIO\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Astronomy Picture of the Day (APOD)<br>Création du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire\n",
    "\n",
    "* [Présentation](#présentation)\n",
    "* [Adresse et clé de l'API](#adresse-et-clé-de-lapi)\n",
    "* [Dossiers de destination des fichiers JSON](#dossiers-de-destination-des-fichiers-json)\n",
    "* [Définition des fonctions nécessaires](#définition-des-fonctions-nécessaires)\n",
    "    * [Requêtes et sauvegarde des données brutes](#requêtes-et-sauvegarde-des-données-brutes)\n",
    "    * [Nettoyage du texte](#nettoyage-du-texte)\n",
    "    * [Ajout des information relatives à l'image](#ajout-des-information-relative-à-limage)\n",
    "    * [Ajout des information relatives au texte](#ajout-des-information-relative-au-texte)\n",
    "    * [Enregistrement du *dataset* au format CSV](#enregistrement-du-dataset-au-format-csv)\n",
    "* [Exécution du code](#exécution-du-code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque jour depuis juin 1995, sur son site [*Astronomy Picture of the Day*](https://apod.nasa.gov/apod/), la NASA présente une image différente de notre univers accompagnée d'une brève explication rédigée par un astronome professionnel.\n",
    "\n",
    "Les métadonnées associées à chaque image sont accessibles *via* l'API dédiée mise à diposition sur le portail [NASA Open APIs](https://api.nasa.gov/).\n",
    "\n",
    "Ce sont ces données qui serviront de base à ce projet. Elles seront ensuite nettoyées et enrichies de nouvelles *variables* pour créer le *dataset* final.\n",
    "\n",
    ">Ce *notebook* contient le code détaillé de l'ensemble de ces étapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adresse et clé de l'API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par initialiser l'adresse de l'API ainsi que la clé personnelle.\n",
    "\n",
    "Si aucune clé n'est fournie, on utilise la clé de démo dont les limites sont :\n",
    "* horaire : 30 requêtes par adresse IP par hour ;\n",
    "* quotidienne : 50 requêtes par adresse IP par jour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définit l'adresse de l'API\n",
    "API_URL = 'https://api.nasa.gov/planetary/apod'\n",
    "\n",
    "# Définit la clé de l'API contenue dans le fichier JSON externe\n",
    "p = Path.cwd()\n",
    "q = p / 'api_key.json'\n",
    "\n",
    "if q.exists():\n",
    "    f = open('./api_key.json')\n",
    "    API_KEY = json.load(f)['api_key']\n",
    "    f.close()\n",
    "\n",
    "if not API_KEY:\n",
    "    API_KEY = 'DEMO_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dossiers de destination des fichiers JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chaque étape du *process*, des fichiers JSON sont générés pour chacune des dates de la période définie dans la requête.\n",
    "\n",
    ">Ce choix n'est pas forcément le plus judicieux en terme d'écriture disque mais ce projet a vocation a être transposé dans **Amazon AWS** et le but est de travailler sur la gestion des ***buckets* S3** et des **Lambda**.\n",
    "\n",
    "Ces fichiers sont enregistrés des dossiers ayant la structure suivante :<br>\n",
    ".<br>\n",
    "└── json/<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;├── raw<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;├── level-0<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;├── level-1<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;└── level-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dossier 'raw' existe déjà.\n",
      "Le dossier 'level-0' existe déjà.\n",
      "Le dossier 'level-1' existe déjà.\n",
      "Le dossier 'level-2' existe déjà.\n"
     ]
    }
   ],
   "source": [
    "# Définit la structure des dossiers de destination des fichiers JSON\n",
    "JSON_FOLDER = 'json'\n",
    "RAW_FOLDER = 'raw'\n",
    "LEVEL0_FOLDER = 'level-0'\n",
    "LEVEL1_FOLDER = 'level-1'\n",
    "LEVEL2_FOLDER = 'level-2'\n",
    "CSV_FOLDER = 'csv'\n",
    "\n",
    "# Crée les dossiers\n",
    "folders_list = [RAW_FOLDER, LEVEL0_FOLDER, LEVEL1_FOLDER, LEVEL2_FOLDER]\n",
    "\n",
    "for folder in folders_list:\n",
    "    p = Path.cwd()\n",
    "    q = p / JSON_FOLDER / folder\n",
    "    if not q.exists():\n",
    "        q.mkdir(parents=True, exist_ok=False)\n",
    "        print(f'''Le dossier '{folder}' a été créé avec succès.''')\n",
    "    else:\n",
    "        print(f'''Le dossier '{folder}' existe déjà.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des fonctions nécessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie contient les fonctions nécessaires à chaque étape du *process*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requêtes et sauvegarde des données brutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque date comprise dans la période définie par `start_date` et `end_date`, une requête est envoyée à l'API et le résultat est enregistré dans un fichier JSON individuel (*niveau RAW*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_json_from_apod_api(\n",
    "        start_date,\n",
    "        end_date,\n",
    "        api_key=API_KEY,\n",
    "        api_url = API_URL\n",
    "    ):\n",
    "\n",
    "    '''\n",
    "    Fonction permettant de requêter sur l'API APOD de la NASA et d'enregistrer\n",
    "    le résultat de chaque requête dans dans un fichier JSON individuel.\n",
    "\n",
    "    Paramètres :\n",
    "    -\n",
    "    - start_date : la date (YYYY-MM-DD) à partir de laquelle récupérer les \n",
    "    informations ;\n",
    "    - end_date : la date (YYYY-MM-DD) à partir de laquelle arrêter de récupérer \n",
    "    les informations ;\n",
    "    - api_key : la clé pour accéder à l'API ;\n",
    "    - api_url : l'adresse de l'API.\n",
    "    '''    \n",
    "\n",
    "    # Convertit les dates au format date si elles ne sont pas vides\n",
    "    if start_date and end_date:\n",
    "        # Vérifie la validité du format des dates\n",
    "        try:\n",
    "            start_date = datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "            end_date = datetime.strptime(end_date, '%Y-%m-%d').date()\n",
    "        except ValueError:\n",
    "            print('Erreur : Les dates ne sont pas au format YYYY-MM-DD.')\n",
    "    else:\n",
    "        print(\"Erreur : Les dates ne doivent pas être vides.\")\n",
    "\n",
    "\n",
    "    # Définit le delta des périodes (attention si delta >  90 jours)\n",
    "    delta = timedelta(days=90)\n",
    "\n",
    "    # Boucle sur la période de dates\n",
    "    while start_date <= end_date:\n",
    "        end_of_period = min(start_date + delta, end_date)\n",
    "        \n",
    "        # Définit les paramètres de la requête\n",
    "        params = {\n",
    "            'api_key': api_key,\n",
    "            'start_date': start_date.strftime('%Y-%m-%d'),\n",
    "            'end_date': end_of_period.strftime('%Y-%m-%d')\n",
    "        }\n",
    "\n",
    "        # Effectue la requête\n",
    "        r = requests.get(api_url, params=params)\n",
    "\n",
    "        # Boucle sur chaque élément du résultat de la requête\n",
    "        for i in range (len(r.json())):\n",
    "            \n",
    "            # Définit le nom du fichier de destination\n",
    "            p = Path.cwd()\n",
    "            q = p / JSON_FOLDER / RAW_FOLDER / (r.json()[i]['date']+'.json')\n",
    "            \n",
    "            # Définit le dictionnaire à écrire\n",
    "            data = r.json()[i]\n",
    "\n",
    "            # Crée le(s) fichier(s) et écrit les données de la requêtes\n",
    "            with open(q, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=True, indent=4)\n",
    "\n",
    "        start_date += delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque fichier enregistré au *niveau RAW*, le texte des variables `copyright`, `explanation` et `title` est nettoyé. Une variable unique contenant l'url du media est créée et les variables inutiles sont supprimés. Enfin, un nouveau fichier est généré au *niveau level 0*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json_file(file_to_process):\n",
    "    '''\n",
    "    Fonction de nettoyage des fichiers JSON issus des requêtes sur l'API APOD\n",
    "    de la NASA.\n",
    "\n",
    "    Paramètre :\n",
    "    -\n",
    "    - file_to_process : le fichier JSON à traiter.\n",
    "    '''\n",
    "\n",
    "    # Importe le fichier\n",
    "    with open(file_to_process, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Définit les clés pour lesquelles nettoyer le texte\n",
    "    keys_to_clean = ['copyright', 'explanation', 'title']\n",
    "\n",
    "    # Boucle sur les clés à nettoyer et applique les règles définies\n",
    "    for key in keys_to_clean:\n",
    "        if key in data:\n",
    "            # Supprime le caractère de nouvelle ligne\n",
    "            data[key] = re.sub('\\n', '', data[key])\n",
    "            # Règle d'espace pour '.' et ','\n",
    "            data[key] = re.sub(r'\\s*([.,])\\s*', r'\\1 ', data[key])\n",
    "            # Règle d'espace pour '?', '!', ':'\n",
    "            data[key] = re.sub(r'\\s*([;!?:])\\s*', r' \\1 ', data[key])\n",
    "            # Règle d'espace '(', '[' et '{'\n",
    "            data[key] = re.sub(r'\\s*([(\\[{])\\s*', r' \\1', data[key])\n",
    "            # Règle d'espace ')', ']' et '}'\n",
    "            data[key] = re.sub(r'\\s*([)\\]}])\\s*', r'\\1 ', data[key])\n",
    "            # Règle d'espace '-' et '/'\n",
    "            data[key] = re.sub(r'\\s*([-/])\\s*', r'\\1', data[key])\n",
    "            # Règle d'espace '--'\n",
    "            data[key] = re.sub(r'--', r' -- ', data[key])\n",
    "            # Supprime les espaces superflus\n",
    "            data[key] = re.sub(r'\\s+', ' ', data[key])\n",
    "            # Supprimer 'Credit and Copyright : '\n",
    "            data[key] = re.sub(r'Credit and Copyright : ', '', data[key])\n",
    "            # Supprimer 'Credit : '\n",
    "            data[key] = re.sub(r'Credit : ', '', data[key]) \n",
    "            # Supprimer l'espace à la fin de la chaîne de caractères\n",
    "            data[key] = data[key].rstrip()\n",
    "\n",
    "    # Supprime 'explanation' de 'copyright' (bug sur images de 1995)\n",
    "    if 'copyright' in data and 'Explanation' in data['copyright']:\n",
    "        data['copyright'] = data['copyright'].split(' Explanation')[0]\n",
    "\n",
    "    # Définit les extensions de fichiers à exclure\n",
    "    extensions_to_exclude = ['mov', 'mpg', 'wmv', 'avi', 'mp4', 'mkv']\n",
    "\n",
    "    # Crée un lien unique 'media_url' dans le dictionnaire\n",
    "    if data['media_type'] == 'image':\n",
    "        if ('hdurl' in data\n",
    "            and data['hdurl'].split('.')[-1] not in extensions_to_exclude):\n",
    "            data['media_url'] = data['hdurl']\n",
    "        elif data['hdurl'].split('.')[-1] in extensions_to_exclude:\n",
    "            data['media_url'] = data['url']\n",
    "        elif 'hdurl' not in data:\n",
    "            data['media_url'] = data['url']\n",
    "    elif data['media_type'] == 'video':\n",
    "        data['media_url'] = data['url']\n",
    "\n",
    "    # Définit les clés à supprimer\n",
    "    keys_to_remove = ['hdurl', 'service_version', 'url']\n",
    "\n",
    "    # Supprime les clés\n",
    "    for key in keys_to_remove:\n",
    "        if key in data:\n",
    "            del data[key]\n",
    "\n",
    "\n",
    "    # Exporte le fichier dans le dossier de destination\n",
    "    p = Path.cwd()\n",
    "    q = p / JSON_FOLDER / LEVEL0_FOLDER / file_to_process.name\n",
    "\n",
    "    with open(q, 'w', encoding='utf-8') as of: \n",
    "        json.dump(data, of, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout des information relative à l'image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque fichier enregistré au *niveau level 0*, et avec **Request** et **Pillow**, on ouvre l'image à partir du lien contenu dans `media_url`. On extrait ensuite les informations de base de l'image puis on extrait les *tags* EXIF. Enfin, un nouveau fichier est enregistré au *niveau level 1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_image_features_to_json(file_to_process):\n",
    "    '''\n",
    "    Fonction permettant d'enrichir les données des fichiers JSON issus des\n",
    "    requêtes sur l'API APOD de la NASA avec des informations sur les images\n",
    "    (si elles sont disponibles) : résolution, mode, format, marque et modèle\n",
    "    de l'appareil photo et logiciel de retouche utilisé.\n",
    "\n",
    "    Paramètre :\n",
    "    -\n",
    "    - file_to_process : le fichier JSON à traiter.\n",
    "    '''\n",
    "\n",
    "    # Importe le fichier\n",
    "    with open(file_to_process, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if data['media_type'] == 'image':\n",
    "\n",
    "        # Ouvre l'image à partir de 'media_url'\n",
    "        response = requests.get(data['media_url'])\n",
    "        \n",
    "        if response.status_code != 404:\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "\n",
    "            # Ajoute les informations de base sur l'image\n",
    "            data['img_width_px'] = img.size[0]\n",
    "            data['img_height_px'] = img.size[1]\n",
    "            data['img_mode'] = img.mode\n",
    "            data['img_format'] = img.format\n",
    "\n",
    "            # Vérifie si l'image a des données EXIF\n",
    "            if hasattr(img, '_getexif') and img._getexif():\n",
    "                # Extrait les données EXIF\n",
    "                exif_data = img._getexif()\n",
    "\n",
    "                # Définit les tags à extraire\n",
    "                tags_to_extract = ['Make', 'Model', 'Software']\n",
    "\n",
    "                # Rechercher les tags dans les données EXIF\n",
    "                extracted_tags = {\n",
    "                    ExifTags.TAGS.get(tag_id, tag_id): value \n",
    "                    for tag_id, value in exif_data.items()\n",
    "                    if ExifTags.TAGS.get(tag_id) in tags_to_extract\n",
    "                }\n",
    "\n",
    "                # Ajoute les informations dictionnaire\n",
    "                for tag_name, value in extracted_tags.items():\n",
    "                    data[tag_name] = value\n",
    "\n",
    "    # Renomme les clés\n",
    "    if 'Make' in data:\n",
    "        data['camera_make'] = data.pop('Make', None)\n",
    "    if 'Model' in data:\n",
    "        data['camera_model'] = data.pop('Model', None)\n",
    "    if 'Software' in data:\n",
    "        data['software'] = data.pop('Software', None)\n",
    "\n",
    "    # Définit les clés pour lesquelles nettoyer le texte\n",
    "    keys_to_clean = ['camera_make', 'camera_model']\n",
    "\n",
    "    # Boucle sur les clés à nettoyer et applique les règles définies\n",
    "    for key in keys_to_clean:\n",
    "        if key in data:\n",
    "            # Supprime le caractère '\\x00'\n",
    "            data[key] = data[key].replace('\\x00', '')\n",
    "            # Supprime les espaces superflus\n",
    "            data[key] = re.sub(r'\\s+', ' ', data[key]) \n",
    "            # Supprimer l'espace à la fin de la chaîne de caractères\n",
    "            data[key] = data[key].rstrip()\n",
    "    \n",
    "    # Exporte le fichier dans le dossier de destination\n",
    "    p = Path.cwd()\n",
    "    q = p / JSON_FOLDER / LEVEL1_FOLDER / file_to_process.name\n",
    "\n",
    "    with open(q, 'w', encoding='utf-8') as of: \n",
    "        json.dump(data, of, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout des information relative au texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque fichier enregistré au *niveau level 1*, on utilise **spaCy** pour extraire les mots clés et les entités de localisation contenus dans la variable `explanation`. Un nouveau fichier est ensuite enregistré au *niveau level 2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_keywords_and_locations(file_to_process, x=20):\n",
    "    '''\n",
    "    Fonction permettant d'enrichir les données des fichiers JSON issus des\n",
    "    requêtes sur l'API APOD de la NASA en extrayant les mots clés et les\n",
    "    entités nommées (lieux) à partir de la variable 'explanation'.\n",
    "\n",
    "    Paramètres :\n",
    "    -\n",
    "    - file_to_process : le fichier JSON à traiter ;\n",
    "    - x : le nombre de mots-clés à conserver dans le top (20 par défaut).\n",
    "    '''\n",
    "\n",
    "    # Importe le fichier\n",
    "    with open(file_to_process, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    # Définit le texte à analyser\n",
    "    text = data['explanation']\n",
    "\n",
    "    # Charge le modèle linguistique en anglais\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Traite le texte avec spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # I. Récupère les entités nommées de localisation\n",
    "\n",
    "    # Initialise un set\n",
    "    named_entities = set()\n",
    "\n",
    "    # Ajoute les entités géopolitiques et les lieux au set\n",
    "    [named_entities.add(ent.text)\n",
    "        for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
    "\n",
    "    # Ajoute le set au dictionnaire 'data'\n",
    "    if len(named_entities) > 0:\n",
    "        data['named_entities'] = (', ').join(sorted(named_entities))\n",
    "    \n",
    "    # II. Récupère les mots clés\n",
    "\n",
    "    # Initialise un dictionnaire\n",
    "    keywords = {}\n",
    "\n",
    "    # Boucle sur les 'tokens' du texte\n",
    "    for token in doc:\n",
    "\n",
    "        # Filtre les 'tokens' en fonction de leur nature\n",
    "        if (\n",
    "            token.is_alpha\n",
    "            and not token.is_stop\n",
    "            and token.pos_ != 'ADV'\n",
    "            and token.ent_type_ not in ['GPE', 'LOC']\n",
    "        ):\n",
    "\n",
    "            # Utilise le lemme du mot\n",
    "            keyword = token.lemma_\n",
    "\n",
    "            # Ajoute le mot et son score au dictionnaire 'keywords' s'il n'est\n",
    "            # pas déjà présent ou s'il a un score plus élevé\n",
    "            if keyword not in keywords or token.rank > keywords[keyword]:\n",
    "                keywords[keyword] = token.rank\n",
    "\n",
    "    # Trie les mots-clés selon leur score\n",
    "    sorted_keywords = sorted(\n",
    "        keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Sélectionne les 'x' premiers résultats\n",
    "    top_keywords = [keyword[0] for keyword in sorted_keywords[:x]]\n",
    "\n",
    "    # Ajoute la liste au dictionnaire 'data'\n",
    "    if len(top_keywords) > 0:\n",
    "        data['keywords'] = (', ').join(top_keywords)\n",
    "\n",
    "    # III. Enregistrement du JSON\n",
    "\n",
    "    # Exporte le fichier dans le dossier de destination\n",
    "    p = Path.cwd()\n",
    "    q = p / JSON_FOLDER / LEVEL2_FOLDER / file_to_process.name\n",
    "\n",
    "    with open(q, 'w', encoding='utf-8') as of: \n",
    "        json.dump(data, of, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enregistrement du *dataset* au format CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les fichiers du *niveau level 2* sont compilés au sein d'un même *DataFrame* puis celui-ci est exporté en fichier CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_dataset_from_json_files(csv_filename='nasa-apod-dataset.csv'):\n",
    "    \n",
    "    '''\n",
    "    Fonction permettant de créer et d'exporter un dataset au format CSV à \n",
    "    partir des fichiers JSON issus des requêtes sur l'API APOD de la NASA qui \n",
    "    ont ensuite été nettoyés et enrichis.\n",
    "\n",
    "    Paramètre :\n",
    "    -\n",
    "    - csv_filename : le nom du fichier CSV à créer (par défaut \n",
    "    'nasa-apod-dataset.csv').\n",
    "    '''\n",
    "\n",
    "    # Initialise un DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\n",
    "            'date',\n",
    "            'title',\n",
    "            'copyright',\n",
    "            'explanation',\n",
    "            'keywords',\n",
    "            'named_entities',\n",
    "            'media_type',\n",
    "            'media_url',\n",
    "            'img_format',\n",
    "            'img_mode',\n",
    "            'img_width_px',\n",
    "            'img_height_px',\n",
    "            'camera_make',\n",
    "            'camera_model',\n",
    "            'software'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Crée la liste des fichiers JSON a lire\n",
    "    p = Path.cwd()\n",
    "    q = p / JSON_FOLDER / LEVEL2_FOLDER\n",
    "    files = Path(q).glob('*.json')\n",
    "\n",
    "    # Charge un fichier 'json' puis l'ajoute au DataFrame\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "            df = pd.concat(\n",
    "                [df, pd.DataFrame.from_dict(data, orient='index').T],\n",
    "                ignore_index=True\n",
    "        )\n",
    "            \n",
    "    # Exporte le DataFrame dans un fichier 'csv'\n",
    "    q = p / CSV_FOLDER / csv_filename\n",
    "    df.to_csv(q, sep=';',  index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exécution du code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Récupère les données\n",
    "    get_raw_json_from_apod_api(start_date='2024-01-01', end_date='2024-01-02')\n",
    "\n",
    "    # Liste les fichiers JSON à traiter\n",
    "    p = Path.cwd()\n",
    "    q = p / JSON_FOLDER / RAW_FOLDER\n",
    "    files = q.glob('*.json')\n",
    "\n",
    "    # Applique les fonctions de nettoyage et enrichissement\n",
    "    for file in files:\n",
    "        clean_json_file(file.parent.parent / RAW_FOLDER / file.name)\n",
    "        add_image_features_to_json(\n",
    "            file.parent.parent / LEVEL0_FOLDER / file.name)\n",
    "        generate_keywords_and_locations(\n",
    "            file.parent.parent / LEVEL1_FOLDER / file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
